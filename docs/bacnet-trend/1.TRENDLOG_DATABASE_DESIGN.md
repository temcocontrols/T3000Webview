# T3000 Trendlog Database Design - Comprehensive Analysis & Solution

**Date:** August 7, 2025
**Branch:** feature/new-ui
**Status:** Design Phase - No Implementation Yet

---

## Executive Summary

This document provides a comprehensive solution for implementing historical data storage in the T3000 WebView system. Currently, the system only provides real-time data through WebView2/WebSocket messages. This design introduces a separate SQLite database specifically for trendlog historical data while preserving existing functionality.

---

## Current System Analysis

### 🏗️ Existing Architecture

```
T3000 C++ Application (Main Process)
├── Hardware Communication (BACnet/Modbus)
│   ├── INPUT Sensors: 64 points per device
│   ├── OUTPUT Actuators: 64 points per device
│   └── VARIABLE Points: 128 points per device
├── t3_webview_api.dll (Rust API Server)
│   ├── WebSocket Server: Port 9104
│   ├── HTTP API: Port 9103
│   └── Database: webview_database.db
└── Vue.js Frontend
    ├── Built-in WebView2 (embedded in T3000)
    ├── External Browser Support (http://localhost:9103)
    └── Real-time Communication Only
```

### 📊 Current Data Flow

```
Hardware Devices → T3000 C++ → Rust API → Vue Frontend
     ↑                ↑           ↑           ↑
BACnet/Modbus    Message Loop   WebSocket    Live Display
   Protocol                    HTTP API     (No History)
```

### 📁 Current Database Usage (webview_database.db)

**Purpose:** Modbus register management and configuration
- `modbus_register` - Modbus register definitions
- `modbus_register_devices` - Device registry
- `modbus_register_settings` - Configuration
- `user` - Authentication data

**Current Tables:**
```sql
modbus_register              -- Register definitions
modbus_register_devices      -- Device registry
modbus_register_settings     -- System settings
user                         -- User authentication
files                        -- File management
```

---

## 🎯 Problem Statement

### Current Limitations
- ❌ **No Historical Data Storage** - Only real-time values available
- ❌ **No Trending Capability** - Cannot view data over time
- ❌ **No Data Analysis** - Missing historical patterns and insights
- ❌ **No Performance Monitoring** - Cannot track system performance trends
- ❌ **No Reporting** - Limited data export and reporting capabilities

### User Requirements
- ✅ **Store historical sensor data** from INPUT, OUTPUT, VARIABLE points
- ✅ **Configurable collection intervals** (1 minute to 4 days)
- ✅ **Real-time trending charts** in Vue.js interface
- ✅ **Data export functionality** (CSV, Excel formats)
- ✅ **Multiple device support** (up to 40 devices)
- ✅ **Background collection** without user interaction
- ✅ **Performance optimization** for large datasets

---

## 🗄️ Proposed Solution: Dual Database Architecture

### Design Philosophy
**Separation of Concerns:** Keep existing webview functionality completely intact while adding new trendlog capabilities through a separate database.

### Database Strategy
```
📁 api/Database/
├── webview_database.db     (EXISTING - Unchanged)
│   ├── Modbus register management
│   ├── User authentication
│   ├── File management
│   └── WebView configuration
│
└── trendlog_database.db    (NEW - Trendlog specific)
    ├── Device management for trending
    ├── Point configuration
    ├── Historical time-series data
    └── Collection settings
```

---

## 🏗️ New Trendlog Database Schema

### Core Tables Design

#### 1. Devices Table
```sql
CREATE TABLE devices (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    device_id INTEGER UNIQUE NOT NULL,           -- T3000 device ID (1-254)
    device_name TEXT NOT NULL,                   -- User-friendly name
    ip_address TEXT,                             -- Device IP address
    device_type TEXT NOT NULL DEFAULT 'T3000',  -- 'T3000', 'SUB_PANEL'
    serial_number INTEGER,                       -- Hardware serial number
    is_active INTEGER DEFAULT 1,                -- 1=active, 0=inactive
    last_seen INTEGER,                           -- Unix timestamp of last communication
    created_at INTEGER DEFAULT (strftime('%s', 'now')),
    updated_at INTEGER DEFAULT (strftime('%s', 'now'))
);

-- Indexes for performance
CREATE INDEX idx_devices_device_id ON devices(device_id);
CREATE INDEX idx_devices_active ON devices(is_active);
CREATE INDEX idx_devices_last_seen ON devices(last_seen);
```

#### 2. Trend Points Table
```sql
CREATE TABLE trend_points (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    device_id INTEGER NOT NULL,                 -- References devices.device_id
    point_type TEXT NOT NULL,                   -- 'INPUT', 'OUTPUT', 'VARIABLE'
    point_number INTEGER NOT NULL,              -- Point number (1-64 for IN/OUT, 1-128 for VAR)
    point_name TEXT,                            -- T3000 point label
    description TEXT,                           -- T3000 point description
    unit TEXT,                                  -- Engineering units
    data_type TEXT NOT NULL DEFAULT 'ANALOG',  -- 'ANALOG', 'DIGITAL'
    is_enabled INTEGER DEFAULT 1,              -- 1=trending enabled, 0=disabled
    collection_interval INTEGER DEFAULT 60,     -- Collection interval in seconds
    min_value REAL,                             -- Expected minimum value
    max_value REAL,                             -- Expected maximum value
    created_at INTEGER DEFAULT (strftime('%s', 'now')),
    updated_at INTEGER DEFAULT (strftime('%s', 'now')),

    FOREIGN KEY (device_id) REFERENCES devices(device_id),
    UNIQUE(device_id, point_type, point_number)
);

-- Indexes for performance
CREATE INDEX idx_trend_points_device ON trend_points(device_id);
CREATE INDEX idx_trend_points_enabled ON trend_points(is_enabled);
CREATE INDEX idx_trend_points_type ON trend_points(point_type);
CREATE INDEX idx_trend_points_interval ON trend_points(collection_interval);
```

#### 3. Trend Data Table (Time-Series Storage)
```sql
CREATE TABLE trend_data (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    point_id INTEGER NOT NULL,                  -- References trend_points.id
    timestamp INTEGER NOT NULL,                 -- Unix timestamp
    value REAL NOT NULL,                        -- Actual data value
    quality INTEGER DEFAULT 0,                  -- Data quality: 0=good, 1=bad, 2=uncertain
    created_at INTEGER DEFAULT (strftime('%s', 'now')),

    FOREIGN KEY (point_id) REFERENCES trend_points(id)
);

-- Critical indexes for time-series queries
CREATE INDEX idx_trend_data_point_time ON trend_data(point_id, timestamp);
CREATE INDEX idx_trend_data_timestamp ON trend_data(timestamp);
CREATE INDEX idx_trend_data_quality ON trend_data(quality);
```

#### 4. Timebase Configuration Table
```sql
CREATE TABLE timebase_config (
    id INTEGER PRIMARY KEY,
    name TEXT NOT NULL UNIQUE,                  -- '1-minute', '5-minutes', etc.
    interval_seconds INTEGER NOT NULL,          -- Interval in seconds
    retention_days INTEGER NOT NULL,            -- How long to keep data
    description TEXT,                           -- Human-readable description
    is_active INTEGER DEFAULT 1,               -- 1=available, 0=disabled
    created_at INTEGER DEFAULT (strftime('%s', 'now'))
);

-- T3000 Standard Timebase Intervals
INSERT INTO timebase_config (id, name, interval_seconds, retention_days, description) VALUES
(1, '1-minute', 60, 7, 'High-resolution trending - 1 week retention'),
(2, '5-minutes', 300, 30, 'Standard trending - 1 month retention'),
(3, '15-minutes', 900, 90, 'Medium-resolution trending - 3 months retention'),
(4, '30-minutes', 1800, 180, 'Lower-resolution trending - 6 months retention'),
(5, '1-hour', 3600, 365, 'Hourly trending - 1 year retention'),
(6, '2-hours', 7200, 730, 'Bi-hourly trending - 2 years retention'),
(7, '4-hours', 14400, 1095, 'Quarterly-daily trending - 3 years retention'),
(8, '1-day', 86400, 1825, 'Daily trending - 5 years retention'),
(9, '4-days', 345600, 3650, 'Weekly trending - 10 years retention');

CREATE INDEX idx_timebase_active ON timebase_config(is_active);
CREATE INDEX idx_timebase_interval ON timebase_config(interval_seconds);
```

#### 5. Collection Status Table
```sql
CREATE TABLE collection_status (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    device_id INTEGER NOT NULL,                 -- References devices.device_id
    point_id INTEGER,                           -- References trend_points.id (NULL = device-level status)
    status TEXT NOT NULL,                       -- 'COLLECTING', 'STOPPED', 'ERROR'
    last_collection INTEGER,                    -- Unix timestamp of last successful collection
    last_error TEXT,                           -- Last error message (if any)
    error_count INTEGER DEFAULT 0,             -- Consecutive error count
    total_collections INTEGER DEFAULT 0,       -- Total successful collections
    created_at INTEGER DEFAULT (strftime('%s', 'now')),
    updated_at INTEGER DEFAULT (strftime('%s', 'now')),

    FOREIGN KEY (device_id) REFERENCES devices(device_id),
    FOREIGN KEY (point_id) REFERENCES trend_points(id)
);

CREATE INDEX idx_collection_device ON collection_status(device_id);
CREATE INDEX idx_collection_status ON collection_status(status);
CREATE INDEX idx_collection_last ON collection_status(last_collection);
```

---

## 🔄 Data Collection Architecture

### Collection Service Design
```
Background Collection Service (Rust)
├── Device Scanner
│   ├── Auto-discover T3000 devices on network
│   ├── Register devices in trendlog database
│   └── Monitor device online/offline status
├── Point Manager
│   ├── Auto-configure trending for active points
│   ├── Apply timebase intervals based on point type
│   └── Enable/disable trending per user configuration
├── Data Collector
│   ├── Schedule collection based on intervals
│   ├── Read point values via T3000 C++ bridge
│   ├── Validate and store data
│   └── Handle communication errors gracefully
└── Real-time Broadcaster
    ├── Send updates via WebSocket (port 9104)
    ├── Notify frontend of new data
    └── Push device status changes
```

### Collection Flow Process
```
1. Service Startup
   ├── Connect to trendlog_database.db
   ├── Load enabled devices and points
   ├── Initialize collection schedules
   └── Start background collection tasks

2. Device Discovery Loop (Every 5 minutes)
   ├── Scan T3000 network for devices
   ├── Compare with database device list
   ├── Add new devices automatically
   └── Update device last_seen timestamps

3. Point Collection Loop (Per configured interval)
   ├── For each enabled point:
   │   ├── Read current value from T3000
   │   ├── Validate value against min/max bounds
   │   ├── Store in trend_data table
   │   └── Broadcast to WebSocket clients
   ├── Update collection_status
   └── Handle and log any errors

4. Maintenance Tasks (Daily)
   ├── Clean up old data based on retention policies
   ├── Optimize database (VACUUM, REINDEX)
   ├── Generate collection statistics
   └── Archive old data if needed
```

---

## 📡 API Design for Trendlog

### REST API Endpoints (Port 9103)

#### Device Management
```http
GET    /api/trendlog/devices                    # List all devices
POST   /api/trendlog/devices                    # Add device to trending
GET    /api/trendlog/devices/{device_id}        # Get device details
PUT    /api/trendlog/devices/{device_id}        # Update device settings
DELETE /api/trendlog/devices/{device_id}        # Remove device from trending
GET    /api/trendlog/devices/{device_id}/status # Get device collection status
```

#### Point Configuration
```http
GET    /api/trendlog/points                     # List all trend points
POST   /api/trendlog/points                     # Configure point for trending
GET    /api/trendlog/points/{point_id}          # Get point configuration
PUT    /api/trendlog/points/{point_id}          # Update point settings
DELETE /api/trendlog/points/{point_id}          # Disable point trending
GET    /api/trendlog/points/device/{device_id}  # Get points for device
```

#### Historical Data Queries
```http
GET    /api/trendlog/data/{point_id}            # Get historical data for point
POST   /api/trendlog/data/query                 # Advanced multi-point queries
GET    /api/trendlog/data/latest/{point_id}     # Get latest value for point
GET    /api/trendlog/data/export                # Export data (CSV/Excel)
```

#### Collection Control
```http
GET    /api/trendlog/collection/status          # Get collection service status
POST   /api/trendlog/collection/start           # Start collection for device/point
POST   /api/trendlog/collection/stop            # Stop collection for device/point
GET    /api/trendlog/collection/statistics      # Get collection statistics
```

#### Timebase Management
```http
GET    /api/trendlog/timebase                   # List available intervals
POST   /api/trendlog/timebase                   # Create custom interval
PUT    /api/trendlog/timebase/{id}              # Update interval settings
```

### WebSocket Messages (Port 9104)

#### Real-time Data Updates
```javascript
// Trend data update
{
  "type": "trend_update",
  "point_id": 123,
  "device_id": 45,
  "timestamp": 1691400000,
  "value": 23.5,
  "quality": 0
}

// Device status change
{
  "type": "device_status",
  "device_id": 45,
  "online": true,
  "last_seen": 1691400000
}

// Collection service status
{
  "type": "collection_status",
  "active_devices": 15,
  "active_points": 380,
  "collections_per_minute": 45.2,
  "errors": []
}
```

---

## 🎨 Frontend Integration

### Vue.js Component Updates

#### TrendLogChart.vue Enhancement
```vue
<template>
  <div class="trendlog-chart">
    <!-- Device and Point Selection -->
    <div class="selection-panel">
      <device-selector
        v-model="selectedDevices"
        :available-devices="availableDevices"
        @selection-change="onDeviceSelectionChange"
      />
      <point-selector
        v-model="selectedPoints"
        :devices="selectedDevices"
        @selection-change="onPointSelectionChange"
      />
    </div>

    <!-- Time Range Controls -->
    <div class="time-controls">
      <time-range-picker
        v-model="timeRange"
        :preset-ranges="presetRanges"
        @range-change="onTimeRangeChange"
      />
      <realtime-toggle
        v-model="realtimeEnabled"
        :update-interval="realtimeInterval"
        @toggle="onRealtimeToggle"
      />
    </div>

    <!-- Chart Display -->
    <div class="chart-container">
      <chart-js
        ref="trendChart"
        :data="chartData"
        :options="chartOptions"
        @zoom="onChartZoom"
        @pan="onChartPan"
      />
    </div>

    <!-- Export and Analysis Tools -->
    <div class="tools-panel">
      <export-controls
        :selected-points="selectedPoints"
        :time-range="timeRange"
        @export="onDataExport"
      />
      <analysis-tools
        :chart-data="chartData"
        @analyze="onDataAnalysis"
      />
    </div>
  </div>
</template>

<script>
import { useTrendlog } from '@/composables/useTrendlog';
import { useWebSocket } from '@/composables/useWebSocket';

export default {
  name: 'TrendLogChart',
  setup() {
    const {
      availableDevices,
      selectedDevices,
      selectedPoints,
      timeRange,
      chartData,
      loadDevices,
      loadHistoricalData,
      exportData
    } = useTrendlog();

    const {
      isConnected,
      connect,
      subscribe,
      unsubscribe
    } = useWebSocket();

    // Real-time data handling
    const handleRealtimeData = (message) => {
      if (message.type === 'trend_update') {
        updateChartData(message);
      }
    };

    return {
      availableDevices,
      selectedDevices,
      selectedPoints,
      timeRange,
      chartData,
      isConnected,
      loadDevices,
      loadHistoricalData,
      exportData,
      handleRealtimeData
    };
  }
};
</script>
```

#### Composables for Trendlog Integration

```javascript
// composables/useTrendlog.js
import { ref, reactive, computed } from 'vue';
import { TrendlogAPI } from '@/api/trendlog';

export function useTrendlog() {
  const availableDevices = ref([]);
  const selectedDevices = ref([]);
  const selectedPoints = ref([]);
  const timeRange = reactive({
    start: new Date(Date.now() - 24 * 60 * 60 * 1000), // 24 hours ago
    end: new Date()
  });
  const chartData = ref([]);

  const api = new TrendlogAPI();

  const loadDevices = async () => {
    try {
      availableDevices.value = await api.getDevices();
    } catch (error) {
      console.error('Failed to load devices:', error);
    }
  };

  const loadHistoricalData = async () => {
    if (selectedPoints.value.length === 0) return;

    try {
      const promises = selectedPoints.value.map(pointId =>
        api.getHistoricalData(pointId, timeRange.start, timeRange.end)
      );
      const results = await Promise.all(promises);
      chartData.value = processChartData(results);
    } catch (error) {
      console.error('Failed to load historical data:', error);
    }
  };

  const exportData = async (format = 'csv') => {
    try {
      const exportQuery = {
        point_ids: selectedPoints.value,
        start_time: timeRange.start.getTime() / 1000,
        end_time: timeRange.end.getTime() / 1000,
        format: format
      };
      const blob = await api.exportData(exportQuery);
      downloadFile(blob, `trendlog_export.${format}`);
    } catch (error) {
      console.error('Failed to export data:', error);
    }
  };

  return {
    availableDevices,
    selectedDevices,
    selectedPoints,
    timeRange,
    chartData,
    loadDevices,
    loadHistoricalData,
    exportData
  };
}
```

---

## ⚡ Performance Optimization

### Database Optimization

#### Indexing Strategy
```sql
-- Primary performance indexes (already included above)
CREATE INDEX idx_trend_data_point_time ON trend_data(point_id, timestamp);
CREATE INDEX idx_trend_data_timestamp ON trend_data(timestamp);

-- Additional composite indexes for common queries
CREATE INDEX idx_trend_data_time_quality ON trend_data(timestamp, quality);
CREATE INDEX idx_trend_points_device_enabled ON trend_points(device_id, is_enabled);
CREATE INDEX idx_devices_active_type ON devices(is_active, device_type);
```

#### Data Retention and Archival
```sql
-- Automatic cleanup trigger for old data
CREATE TRIGGER cleanup_old_trend_data
AFTER INSERT ON trend_data
WHEN (SELECT COUNT(*) FROM trend_data) > 1000000  -- 1M records threshold
BEGIN
  DELETE FROM trend_data
  WHERE timestamp < (strftime('%s', 'now') -
    (SELECT MIN(retention_days * 86400) FROM timebase_config WHERE is_active = 1));
END;
```

#### Query Optimization
```rust
// Efficient batch queries for large datasets
pub async fn get_trend_data_optimized(
    conn: &DatabaseConnection,
    point_ids: Vec<u32>,
    start_time: i64,
    end_time: i64,
    max_points: Option<u32>
) -> Result<Vec<TrendDataPoint>, DbErr> {
    let max_points = max_points.unwrap_or(10000);

    // Calculate optimal sampling interval for large datasets
    let time_span = end_time - start_time;
    let estimated_points = point_ids.len() as u32 * (time_span / 60) as u32; // Assume 1-minute intervals

    let query = if estimated_points > max_points {
        // Use sampling for large datasets
        let sample_interval = estimated_points / max_points;
        trend_data::Entity::find()
            .filter(trend_data::Column::PointId.is_in(point_ids))
            .filter(trend_data::Column::Timestamp.between(start_time, end_time))
            .filter(Expr::expr(Func::cust(format!("id % {}", sample_interval)).eq(0)))
    } else {
        // Return all data for smaller datasets
        trend_data::Entity::find()
            .filter(trend_data::Column::PointId.is_in(point_ids))
            .filter(trend_data::Column::Timestamp.between(start_time, end_time))
    };

    query.all(conn).await
}
```

### Collection Performance

#### Batch Collection Strategy
```rust
// Collect multiple points per device in single operation
pub async fn collect_device_batch(device_id: u32) -> Result<Vec<PointData>, Error> {
    let points = get_enabled_points_for_device(device_id).await?;
    let mut results = Vec::new();

    // Group points by collection interval for efficiency
    let mut interval_groups: HashMap<u32, Vec<TrendPoint>> = HashMap::new();
    for point in points {
        interval_groups
            .entry(point.collection_interval)
            .or_insert_with(Vec::new)
            .push(point);
    }

    // Collect each interval group
    for (interval, group_points) in interval_groups {
        if should_collect_now(interval) {
            let batch_results = read_points_batch(device_id, group_points).await?;
            results.extend(batch_results);
        }
    }

    Ok(results)
}
```

#### Error Handling and Retry Logic
```rust
pub async fn collect_with_retry(
    device_id: u32,
    point: &TrendPoint,
    max_retries: u32
) -> Result<PointData, CollectionError> {
    let mut retries = 0;

    loop {
        match read_device_point(device_id, &point).await {
            Ok(data) => return Ok(data),
            Err(error) => {
                retries += 1;
                if retries >= max_retries {
                    // Log error and mark point as having issues
                    update_collection_status(
                        device_id,
                        Some(point.id),
                        CollectionStatus::Error(error.to_string())
                    ).await?;
                    return Err(error);
                }

                // Exponential backoff
                let delay = Duration::from_millis(100 * 2_u64.pow(retries - 1));
                tokio::time::sleep(delay).await;
            }
        }
    }
}
```

---

## 🔒 Security and Reliability

### Data Integrity
```sql
-- Constraints to ensure data integrity
ALTER TABLE trend_data ADD CONSTRAINT chk_timestamp
CHECK (timestamp > 0 AND timestamp <= strftime('%s', 'now'));

ALTER TABLE trend_data ADD CONSTRAINT chk_quality
CHECK (quality IN (0, 1, 2));  -- 0=good, 1=bad, 2=uncertain

-- Foreign key constraints (enabled with PRAGMA)
PRAGMA foreign_keys = ON;
```

### Backup and Recovery
```rust
pub async fn backup_trendlog_database() -> Result<(), Error> {
    let timestamp = chrono::Utc::now().format("%Y%m%d_%H%M%S");
    let backup_path = format!("Database/backups/trendlog_backup_{}.db", timestamp);

    // Use SQLite backup API for consistent backup
    let conn = get_trendlog_connection().await?;
    conn.execute(&format!("VACUUM INTO '{}'", backup_path), []).await?;

    // Keep only last 7 days of backups
    cleanup_old_backups("Database/backups/", 7).await?;

    Ok(())
}
```

### Access Control
```rust
// API endpoint security
pub async fn verify_trendlog_access(
    user: &User,
    action: TrendlogAction,
    resource: TrendlogResource
) -> Result<bool, AuthError> {
    match action {
        TrendlogAction::Read => {
            // All authenticated users can read trending data
            Ok(user.is_authenticated())
        },
        TrendlogAction::Configure => {
            // Only admin users can configure trending
            Ok(user.is_admin())
        },
        TrendlogAction::Export => {
            // Users with export permission can export data
            Ok(user.has_permission("trendlog_export"))
        }
    }
}
```

---

## 📈 Testing Strategy

### Unit Tests
```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_trendlog_database_operations() {
        let conn = setup_test_database().await;

        // Test device CRUD operations
        let device = create_test_device().await;
        assert!(device.id > 0);

        // Test point configuration
        let point = create_test_trend_point(device.device_id).await;
        assert!(point.is_enabled);

        // Test data storage and retrieval
        let data_points = generate_test_data(point.id, 100).await;
        store_trend_data_batch(&conn, data_points).await.unwrap();

        let retrieved = get_trend_data(&conn, point.id, 0, i64::MAX).await.unwrap();
        assert_eq!(retrieved.len(), 100);
    }

    #[tokio::test]
    async fn test_collection_service() {
        let service = CollectionService::new().await;

        // Test device discovery
        let devices = service.discover_devices().await.unwrap();
        assert!(!devices.is_empty());

        // Test data collection
        let collected = service.collect_device_data(devices[0].device_id).await.unwrap();
        assert!(!collected.is_empty());
    }

    #[tokio::test]
    async fn test_api_endpoints() {
        let app = create_test_app().await;

        // Test device listing
        let resp = app.get("/api/trendlog/devices").send().await;
        assert_eq!(resp.status(), StatusCode::OK);

        // Test historical data query
        let resp = app.get("/api/trendlog/data/1?start=0&end=9999999999").send().await;
        assert_eq!(resp.status(), StatusCode::OK);
    }
}
```

### Integration Tests
```rust
#[tokio::test]
async fn test_full_system_integration() {
    // Start the full system
    let _server = start_test_server().await;
    let collector = start_collection_service().await;

    // Simulate device data
    let device_id = 100;
    simulate_device_online(device_id).await;

    // Wait for automatic discovery and configuration
    tokio::time::sleep(Duration::from_secs(10)).await;

    // Verify device was discovered
    let devices = get_devices().await.unwrap();
    assert!(devices.iter().any(|d| d.device_id == device_id));

    // Verify data collection started
    tokio::time::sleep(Duration::from_secs(65)).await; // Wait for collection cycle

    let data = get_latest_data_for_device(device_id).await.unwrap();
    assert!(!data.is_empty());
}
```

---

## 🚀 Deployment and Migration

### Migration Strategy
```rust
// Migration: Add trendlog functionality without affecting existing system
pub async fn migrate_to_trendlog() -> Result<(), MigrationError> {
    // 1. Create trendlog database with schema
    create_trendlog_database().await?;

    // 2. Initialize timebase configuration
    initialize_default_timebase().await?;

    // 3. Discover and register existing devices
    discover_and_register_devices().await?;

    // 4. Start collection service
    start_collection_service().await?;

    // 5. Update API routes to include trendlog endpoints
    register_trendlog_routes().await?;

    Ok(())
}
```

### Configuration
```toml
# .env configuration for trendlog
TRENDLOG_DATABASE_PATH="Database/trendlog_database.db"
TRENDLOG_COLLECTION_ENABLED=true
TRENDLOG_DEFAULT_INTERVAL=60
TRENDLOG_MAX_RETENTION_DAYS=365
TRENDLOG_BACKUP_ENABLED=true
TRENDLOG_BACKUP_INTERVAL_HOURS=24
```

### Monitoring and Alerting
```rust
pub struct TrendlogMonitor {
    collection_rate: f64,
    error_rate: f64,
    database_size: u64,
    active_points: u32,
}

impl TrendlogMonitor {
    pub async fn check_health(&self) -> HealthStatus {
        let mut issues = Vec::new();

        if self.collection_rate < 0.9 {  // Less than 90% success rate
            issues.push("Low collection success rate".to_string());
        }

        if self.error_rate > 0.05 {  // More than 5% error rate
            issues.push("High error rate detected".to_string());
        }

        if self.database_size > 1_000_000_000 {  // > 1GB
            issues.push("Database size growing large".to_string());
        }

        if issues.is_empty() {
            HealthStatus::Healthy
        } else {
            HealthStatus::Warning(issues)
        }
    }
}
```

---

## 📋 Implementation Checklist

### Phase 1: Database Foundation ✅
- [ ] Create trendlog_database.db schema
- [ ] Implement dual database connection management
- [ ] Create Sea-ORM entities for trendlog tables
- [ ] Set up database migrations
- [ ] Add indexes and constraints for performance

### Phase 2: Collection Service ✅
- [ ] Implement T3000 device discovery
- [ ] Create background collection service
- [ ] Add point configuration management
- [ ] Implement data validation and storage
- [ ] Add error handling and retry logic

### Phase 3: API Development ✅
- [ ] Create REST API endpoints for trendlog
- [ ] Implement WebSocket real-time updates
- [ ] Add data export functionality
- [ ] Implement query optimization
- [ ] Add authentication and authorization

### Phase 4: Frontend Integration ✅
- [ ] Update TrendLogChart.vue component
- [ ] Implement real-time data subscription
- [ ] Add historical data visualization
- [ ] Create device and point selection UI
- [ ] Add data export interface

### Phase 5: Testing and Deployment ✅
- [ ] Write comprehensive unit tests
- [ ] Implement integration tests
- [ ] Performance testing with large datasets
- [ ] Security testing and validation
- [ ] Documentation and user guides

---

## 🎯 Success Metrics

### Functional Metrics
- **Data Collection Rate**: > 99% successful collections
- **Query Response Time**: < 5 seconds for typical queries
- **Real-time Latency**: < 1 second from collection to display
- **System Availability**: > 99.9% uptime
- **Data Accuracy**: 100% data integrity

### Performance Metrics
- **Concurrent Users**: Support 10+ simultaneous users
- **Device Capacity**: Handle 40 devices with 256 points each
- **Data Volume**: Efficiently manage years of historical data
- **Memory Usage**: < 500MB additional RAM usage
- **Disk Space**: Efficient storage with automatic cleanup

### User Experience Metrics
- **Setup Time**: < 5 minutes to configure first device
- **Learning Curve**: Intuitive interface requiring minimal training
- **Export Speed**: Export data in < 30 seconds for typical queries
- **Chart Responsiveness**: Smooth interaction with large datasets

---

## 📚 Documentation Plan

### Technical Documentation
- [ ] **API Reference**: Complete REST API documentation with examples
- [ ] **Database Schema**: Detailed table descriptions and relationships
- [ ] **Collection Service**: Architecture and configuration guide
- [ ] **Performance Tuning**: Optimization guidelines and best practices

### User Documentation
- [ ] **Quick Start Guide**: Get trending working in 5 minutes
- [ ] **Configuration Manual**: Device and point setup procedures
- [ ] **Troubleshooting Guide**: Common issues and solutions
- [ ] **Advanced Features**: Power user capabilities and tips

### Developer Documentation
- [ ] **Integration Guide**: How to extend trendlog functionality
- [ ] **Custom Intervals**: Creating custom collection timebase
- [ ] **Plugin Architecture**: Adding custom data sources
- [ ] **Migration Guide**: Upgrading from previous versions

---

## 🔮 Future Enhancements

### Phase 2 Features (v2.0)
- **Data Analytics**: Statistical analysis and pattern detection
- **Alerting System**: Threshold-based notifications and alarms
- **Mobile App**: Dedicated mobile interface for trending
- **Cloud Sync**: Optional cloud backup and synchronization
- **Machine Learning**: Predictive analytics and anomaly detection

### Phase 3 Features (v3.0)
- **Multi-Site Support**: Centralized trending across multiple locations
- **Advanced Reporting**: Automated report generation and scheduling
- **Integration APIs**: Third-party system integration capabilities
- **Custom Dashboards**: User-configurable dashboard layouts
- **Data Federation**: Aggregate data from multiple T3000 systems

---

**Document Version:** 1.0
**Last Updated:** August 7, 2025
**Author:**
**Review Status:** ✅ Ready for Implementation
